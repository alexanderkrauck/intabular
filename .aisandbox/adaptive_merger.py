#!/usr/bin/env python3
"""
Intelligent Data Ingestion: LLM-powered CSV mapping and ingestion system.
Maps unknown CSV structures to well-defined target table schemas automatically.

This system:
1. Analyzes unknown CSV files to understand their semantic structure
2. Maps CSV columns to target table schema using LLM intelligence  
3. Applies intelligent transformations and data quality rules
4. Ingests data while respecting target table policies and constraints
5. Handles schema evolution and data enrichment automatically

Key Use Case:
- You have a well-structured "master" table with detailed YAML configuration
- New data comes in from various sources with unknown/different column structures
- System intelligently maps and ingests the new data into your target schema
"""

import json
import os
import sys
import yaml
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, asdict
import pandas as pd
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

@dataclass
class TableConfig:
    """Configuration for a table's semantic purpose and merge behavior"""
    purpose: str
    enrichment_columns: Union[List[str], Dict[str, str]]  # Support both formats
    column_policy: str  # Free-form text describing data retention policy
    target: Optional[str] = None  # Target file path where data should be saved
    
    @classmethod
    def from_yaml(cls, yaml_path: str) -> 'TableConfig':
        """Load configuration from YAML file"""
        with open(yaml_path, 'r') as f:
            data = yaml.safe_load(f)
        
        # Handle enrichment_columns - support both list and dict formats
        enrichment_columns = data.get('enrichment_columns', [])
        
        # Handle column_policy as free-form text
        column_policy = data.get('column_policy', 'Balance between completeness and conciseness')
        
        # Get target file path
        target = data.get('target', None)
        
        return cls(
            purpose=data['purpose'],
            enrichment_columns=enrichment_columns,
            column_policy=column_policy,
            target=target
        )
    
    def to_yaml(self, yaml_path: str):
        """Save configuration to YAML file"""
        data = {
            'purpose': self.purpose,
            'enrichment_columns': self.enrichment_columns,
            'column_policy': self.column_policy
        }
        
        if self.target:
            data['target'] = self.target
        
        with open(yaml_path, 'w') as f:
            yaml.dump(data, f, default_flow_style=False)
    
    def get_enrichment_column_names(self) -> List[str]:
        """Get list of enrichment column names regardless of format"""
        if isinstance(self.enrichment_columns, dict):
            return list(self.enrichment_columns.keys())
        return self.enrichment_columns
    
    def get_enrichment_column_description(self, column_name: str) -> str:
        """Get description for an enrichment column"""
        if isinstance(self.enrichment_columns, dict):
            return self.enrichment_columns.get(column_name, f"Enrichment field: {column_name}")
        return f"Enrichment field: {column_name}"
    
    def get_column_policy_text(self) -> str:
        """Get column policy as descriptive text"""
        return str(self.column_policy)

class AdaptiveMerger:
    def __init__(self, api_key: Optional[str] = None):
        """Initialize the adaptive merger with OpenAI API key"""
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        
        if not self.api_key:
            raise ValueError("‚ùå OpenAI API key is required! Please set OPENAI_API_KEY in .env file")
        
        try:
            self.client = OpenAI(api_key=self.api_key)
            print("‚úÖ Using OpenAI LLM for intelligent data ingestion")
            
            # Test the API connection
            test_response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": "Test connection"}],
                max_tokens=5
            )
            print(f"üîó API connection verified")
            
        except Exception as e:
            raise RuntimeError(f"‚ùå Failed to initialize OpenAI client: {e}")
    
    def analyze_unknown_csv(self, csv_path: str) -> Dict[str, Any]:
        """Analyze unknown CSV structure and infer semantic meaning of columns"""
        
        # Read sample data
        df = pd.read_csv(csv_path)
        sample_data = df.head(10).to_dict('records')
        
        print(f"üìä CSV File Analysis: {Path(csv_path).name}")
        print(f"   üìà Dimensions: {len(df)} rows √ó {len(df.columns)} columns")
        print(f"   üìã Columns: {', '.join(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}")
        
        # Build detailed column analysis with sample data
        column_details = []
        for col in df.columns:
            sample_values = df[col].dropna().head(3).tolist()
            non_null_count = df[col].notna().sum()
            completeness = (non_null_count / len(df)) * 100
            
            column_details.append({
                "name": col,
                "sample_values": sample_values,
                "completeness_percent": round(completeness, 1),
                "data_type": str(df[col].dtype),
                "unique_values": df[col].nunique()
            })
        
        print(f"   üß† Requesting LLM semantic analysis...")
        
        prompt = f"""
        Analyze this unknown CSV file by examining both column names AND actual data content:
        
        File: {Path(csv_path).name}
        Total Rows: {len(df)}
        
        Detailed Column Analysis:
        {json.dumps(column_details, indent=2)}
        
        For each column, analyze:
        1. Column name semantics (what the name suggests)
        2. Actual data content (what the values reveal)
        3. Business context (what this data represents)
        
        Provide analysis in this JSON format:
        {{
            "table_purpose": "inferred_business_purpose_based_on_all_columns_and_data",
            "data_source": "likely_platform_or_system_this_came_from",
            "column_semantics": {{
                "column_name": {{
                    "semantic_type": "email|name|company|phone|address|identifier|text|number|date|url|social|industry|title|location|other",
                    "confidence": 0.95,
                    "description": "what_this_column_represents_based_on_name_and_data",
                    "data_pattern": "observed_pattern_in_actual_values",
                    "business_value": "how_valuable_this_data_is_for_business_use",
                    "data_quality": "assessment_based_on_completeness_and_consistency"
                }}
            }},
            "data_patterns": {{
                "primary_entity": "what_each_row_represents_based_on_data_analysis",
                "identifier_candidates": ["columns_that_could_uniquely_identify_records"],
                "contact_info": ["email_phone_address_columns"],
                "personal_data": ["name_title_demographic_columns"],
                "business_data": ["company_job_industry_columns"],
                "behavioral_data": ["engagement_activity_preference_columns"],
                "metadata": ["system_timestamp_source_columns"]
            }},
            "quality_assessment": {{
                "overall_completeness": "percentage_of_complete_data",
                "data_consistency": "how_consistent_the_formats_are",
                "potential_duplicates": "likelihood_of_duplicate_records",
                "enrichment_level": "how_enriched_this_data_appears"
            }},
            "source_identification": {{
                "platform_confidence": 0.85,
                "platform_indicators": ["specific_patterns_that_suggest_the_source"],
                "export_type": "contact_list|lead_export|crm_export|social_export|other"
            }}
        }}
        
        Focus on understanding what each column actually contains by examining the real data values.
        Consider data patterns, naming conventions, and business context.
        Return only valid JSON, no explanations.
        """
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1
            )
            
            # Better error handling for API response
            if not response.choices or not response.choices[0].message.content:
                raise ValueError("Empty response from OpenAI API")
            
            response_text = response.choices[0].message.content.strip()
            if not response_text:
                raise ValueError("Empty content in OpenAI API response")
            
            print(f"   ü§ñ LLM analysis received ({len(response_text)} characters)")
            
            # Strip markdown code blocks if present
            if response_text.startswith('```json'):
                response_text = response_text[7:]  # Remove ```json
            if response_text.startswith('```'):
                response_text = response_text[3:]  # Remove ```
            if response_text.endswith('```'):
                response_text = response_text[:-3]  # Remove trailing ```
            response_text = response_text.strip()
            
            try:
                analysis = json.loads(response_text)
            except json.JSONDecodeError as e:
                print(f"   ‚ùå Failed to parse LLM response as JSON: {e}")
                print(f"   üìù Raw response preview: {response_text[:200]}...")
                raise ValueError(f"Invalid JSON response from LLM: {e}")
            
            analysis['file_path'] = csv_path
            analysis['row_count'] = len(df)
            analysis['column_count'] = len(df.columns)
            
            # Log analysis results
            print(f"‚úÖ Semantic Analysis Complete:")
            print(f"   üéØ Business purpose: {analysis.get('table_purpose', 'Unknown')}")
            print(f"   üì¶ Identified source: {analysis.get('data_source', 'Unknown')}")
            
            if 'source_identification' in analysis:
                platform = analysis['source_identification'].get('platform_confidence', 0)
                export_type = analysis['source_identification'].get('export_type', 'unknown')
                print(f"   üîç Platform detection: {platform:.2f} confidence ({export_type})")
            
            if 'quality_assessment' in analysis:
                completeness = analysis['quality_assessment'].get('overall_completeness', 'unknown')
                consistency = analysis['quality_assessment'].get('data_consistency', 'unknown')
                print(f"   üìä Data quality: {completeness} complete, {consistency} consistent")
            
            return analysis
            
        except Exception as e:
            print(f"   ‚ùå LLM analysis failed: {e}")
            raise RuntimeError(f"Cannot proceed without LLM analysis. Error: {e}")
    
    def create_ingestion_strategy(self, target_config: TableConfig, unknown_analysis: Dict) -> Dict[str, Any]:
        """Create strategy to ingest unknown CSV into target table structure"""
        
        print(f"üß† Creating field-by-field ingestion strategy...")
        
        # Build target context
        target_context = self._build_target_context(target_config)
        
        prompt = f"""
        Create a detailed field-by-field ingestion strategy to map unknown CSV data into target table structure:
        
        TARGET TABLE SPECIFICATION:
        {target_context}
        
        UNKNOWN SOURCE DATA ANALYSIS:
        File: {unknown_analysis.get('file_path', 'unknown')}
        Purpose: {unknown_analysis.get('table_purpose', 'unknown')}
        Data Source: {unknown_analysis.get('data_source', 'unknown')}
        
        Source Column Details:
        {json.dumps(unknown_analysis.get('column_semantics', {}), indent=2)}
        
        For each target column, determine the best merge strategy. Available strategies:
        - "replace": Completely replace target column with mapped source column
        - "prompt_merge": Use LLM to intelligently combine target and source data
        - "concat": Concatenate target and source values with separator
        - "derive": Create new value by combining multiple source columns
        - "preserve": Keep existing target data, ignore source
        - "transform": Apply specific transformation (regex, format, etc.)
        
        Create strategy in this JSON format:
        {{
            "field_strategies": {{
                "target_column_name": {{
                    "strategy": "replace|prompt_merge|concat|derive|preserve|transform",
                    "source_mapping": ["source_column1", "source_column2"],
                    "confidence": 0.95,
                    "reasoning": "why_this_strategy_was_chosen",
                    "transformation_rule": "if_transform_strategy_what_rule_to_apply",
                    "prompt_template": "if_prompt_merge_the_LLM_prompt_to_use",
                    "fallback_strategy": "what_to_do_if_primary_strategy_fails"
                }}
            }},
            "unmapped_source_columns": {{
                "source_column": {{
                    "reason": "why_this_column_was_not_mapped",
                    "potential_use": "possible_future_use_for_this_data",
                    "action": "ignore|store_as_metadata|create_new_target_column"
                }}
            }},
            "data_quality_rules": {{
                "deduplication": {{
                    "strategy": "email_based|name_company_based|custom",
                    "key_fields": ["fields_to_use_for_duplicate_detection"],
                    "resolution": "prefer_target|prefer_source|merge_both"
                }},
                "validation": {{
                    "email_validation": true,
                    "phone_formatting": true,
                    "name_standardization": true
                }},
                "cleanup": {{
                    "trim_whitespace": true,
                    "standardize_formats": true,
                    "handle_nulls": "preserve|replace_with_empty|skip"
                }}
            }},
            "ingestion_plan": {{
                "processing_order": ["order_of_field_processing"],
                "conflict_resolution": "how_to_handle_data_conflicts",
                "error_handling": "continue|stop|log_and_continue",
                "validation_checks": ["checks_to_perform_after_ingestion"]
            }},
            "confidence_score": 0.92,
            "risk_assessment": {{
                "data_loss_risk": "low|medium|high",
                "quality_impact": "positive|neutral|negative",
                "schema_compatibility": "excellent|good|fair|poor"
            }},
            "execution_summary": {{
                "total_fields_mapped": 5,
                "fields_requiring_llm": 2,
                "complex_transformations": 1,
                "estimated_processing_time": "fast|medium|slow"
            }}
        }}
        
        Consider:
        - Target column descriptions and business requirements
        - Source data quality and semantic meaning
        - Best strategy for each individual field
        - How to handle conflicts and missing data
        - Data validation and quality preservation
        
        Return only valid JSON, no explanations.
        """
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1
            )
            
            # Better error handling for API response
            if not response.choices or not response.choices[0].message.content:
                raise ValueError("Empty response from OpenAI API")
            
            response_text = response.choices[0].message.content.strip()
            if not response_text:
                raise ValueError("Empty content in OpenAI API response")
            
            print(f"   ü§ñ Strategy response received ({len(response_text)} characters)")
            
            # Strip markdown code blocks if present
            if response_text.startswith('```json'):
                response_text = response_text[7:]  # Remove ```json
            if response_text.startswith('```'):
                response_text = response_text[3:]  # Remove ```
            if response_text.endswith('```'):
                response_text = response_text[:-3]  # Remove trailing ```
            response_text = response_text.strip()
            
            try:
                strategy = json.loads(response_text)
            except json.JSONDecodeError as e:
                print(f"   ‚ùå Failed to parse strategy response as JSON: {e}")
                print(f"   üìù Raw response: {response_text[:200]}...")
                raise ValueError(f"Invalid JSON response from LLM: {e}")
            
            # Log strategy summary
            if 'field_strategies' in strategy:
                print(f"   üìã Field mapping strategies:")
                for target_col, field_strategy in strategy['field_strategies'].items():
                    strategy_type = field_strategy.get('strategy', 'unknown')
                    confidence = field_strategy.get('confidence', 0)
                    print(f"      ‚Ä¢ {target_col}: {strategy_type} (confidence: {confidence:.2f})")
            
            if 'unmapped_source_columns' in strategy:
                unmapped_count = len(strategy['unmapped_source_columns'])
                if unmapped_count > 0:
                    print(f"   ‚ö†Ô∏è  {unmapped_count} source columns will not be mapped")
            
            overall_confidence = strategy.get('confidence_score', 0)
            print(f"   üéØ Overall strategy confidence: {overall_confidence:.2f}")
            
            return strategy
            
        except Exception as e:
            print(f"   ‚ùå Strategy creation failed: {e}")
            raise RuntimeError(f"Cannot proceed without LLM strategy. Error: {e}")
    
    def _build_target_context(self, config: TableConfig) -> str:
        """Build detailed context for target table"""
        
        context = f"Purpose: {config.purpose}\n"
        context += f"Column Policy: {config.get_column_policy_text()}\n\n"
        
        context += "Target Column Specifications:\n"
        if isinstance(config.enrichment_columns, dict):
            for col, desc in config.enrichment_columns.items():
                context += f"  - {col}: {desc}\n"
        else:
            for col in config.enrichment_columns:
                context += f"  - {col}: Important field\n"
        
        return context
    
    def ingest_csv(self, yaml_config_file: str, csv_to_ingest: str) -> pd.DataFrame:
        """
        Complete pipeline to intelligently ingest unknown CSV into target table structure
        
        Args:
            yaml_config_file: Path to YAML configuration file defining target schema and output location
            csv_to_ingest: Path to unknown CSV file to be ingested
            
        Returns:
            DataFrame with ingested data mapped to target schema
        """
        
        print("üöÄ Starting intelligent CSV ingestion pipeline")
        print(f"üìã Config: {yaml_config_file}")
        print(f"üìÑ Input CSV: {csv_to_ingest}")
        
        # Load target configuration from YAML
        if not Path(yaml_config_file).exists():
            raise FileNotFoundError(f"‚ùå YAML configuration file not found: {yaml_config_file}")
        
        print(f"üìù Loading target schema configuration...")
        target_config = TableConfig.from_yaml(yaml_config_file)
        print(f"   üéØ Purpose: {target_config.purpose[:80]}...")
        print(f"   üìä Target columns: {len(target_config.get_enrichment_column_names())}")
        print(f"   üìú Policy: {target_config.column_policy}")
        
        # Get output file from YAML config
        output_file = target_config.target
        if not output_file:
            output_file = "ingested_data.csv"
            print(f"   ‚ö†Ô∏è  No target specified in YAML, using default: {output_file}")
        else:
            print(f"   üìÇ Target file: {output_file}")
        
        # Determine target table file (check if exists or create empty)
        target_columns = target_config.get_enrichment_column_names()
        target_table_file = f"target_table_temp.csv"
        
        # Create empty target table structure
        empty_target = pd.DataFrame(columns=target_columns)
        empty_target.to_csv(target_table_file, index=False)
        print(f"üìã Created target table structure with {len(target_columns)} columns")
        
        # Analyze unknown CSV
        print(f"üîç Analyzing unknown CSV structure...")
        unknown_analysis = self.analyze_unknown_csv(csv_to_ingest)
        
        print(f"‚úÖ CSV Analysis Complete:")
        print(f"   üìä Purpose: {unknown_analysis.get('table_purpose', 'Unknown')}")
        print(f"   üì¶ Source: {unknown_analysis.get('data_source', 'Unknown')}")
        print(f"   üìã Input columns: {unknown_analysis.get('column_count', 0)}")
        print(f"   üìà Input rows: {unknown_analysis.get('row_count', 0)}")
        
        # Create ingestion strategy
        print(f"üß† Creating intelligent field-mapping strategy...")
        strategy = self.create_ingestion_strategy(target_config, unknown_analysis)
        
        strategy_confidence = strategy.get('confidence_score', 0)
        field_count = len(strategy.get('field_strategies', {}))
        unmapped_count = len(strategy.get('unmapped_source_columns', {}))
        
        print(f"‚úÖ Strategy Created:")
        print(f"   üéØ Overall confidence: {strategy_confidence:.2f}")
        print(f"   üó∫Ô∏è  Target fields mapped: {field_count}")
        print(f"   ‚ö†Ô∏è  Source columns unmapped: {unmapped_count}")
        
        # Execute ingestion
        print(f"üîÄ Executing intelligent field-by-field ingestion...")
        ingested_df = self.execute_ingestion(target_table_file, csv_to_ingest, strategy, target_config)
        
        # Clean up temporary file
        if Path(target_table_file).exists():
            os.remove(target_table_file)
        
        # Save result to target specified in YAML
        ingested_df.to_csv(output_file, index=False)
        print(f"üíæ Results saved to: {output_file}")
        
        final_rows = len(ingested_df)
        final_cols = len(ingested_df.columns)
        
        print(f"‚úÖ Ingestion Pipeline Complete!")
        print(f"   üìä Final output: {final_rows} rows √ó {final_cols} columns")
        print(f"   üéØ Schema compliance: Target structure maintained")
        print(f"   üìà Success rate: {strategy_confidence:.1%}")
        
        return ingested_df
    
    def execute_ingestion(self, target_table: str, unknown_csv: str, 
                         strategy: Dict[str, Any], target_config: TableConfig) -> pd.DataFrame:
        """Execute the ingestion strategy with detailed field-by-field processing"""
        
        print(f"üîß Field-by-Field Processing Engine Starting...")
        
        # Load data
        if Path(target_table).exists():
            target_df = pd.read_csv(target_table)
            print(f"   üìä Target structure: {len(target_df)} rows, {len(target_df.columns)} columns")
        else:
            print(f"   üìä Target structure: Creating new schema")
            target_df = pd.DataFrame()
        
        source_df = pd.read_csv(unknown_csv)
        print(f"   üìä Source data: {len(source_df)} rows, {len(source_df.columns)} columns")
        
        # Process each target field according to its strategy
        field_strategies = strategy.get('field_strategies', {})
        result_df = target_df.copy() if not target_df.empty else pd.DataFrame()
        
        total_fields = len(field_strategies)
        print(f"   üéØ Processing {total_fields} target schema fields...")
        
        processed_fields = 0
        for target_col, field_strategy in field_strategies.items():
            processed_fields += 1
            strategy_type = field_strategy.get('strategy', 'unknown')
            source_mapping = field_strategy.get('source_mapping', [])
            confidence = field_strategy.get('confidence', 0)
            reasoning = field_strategy.get('reasoning', 'No reasoning provided')
            
            print(f"\n   [{processed_fields}/{total_fields}] Processing: {target_col}")
            print(f"      üéØ Strategy: {strategy_type}")
            print(f"      üìà Confidence: {confidence:.2f}")
            print(f"      üß† Reasoning: {reasoning[:60]}...")
            
            if source_mapping:
                print(f"      üîó Source mapping: {source_mapping}")
                
                # Check if source columns exist
                available_sources = [col for col in source_mapping if col in source_df.columns]
                if not available_sources:
                    print(f"      ‚ùå No mapped source columns found - preserving existing")
                    continue
                
                print(f"      ‚úÖ Available sources: {available_sources}")
                
                # Apply the specific strategy
                try:
                    if strategy_type == "replace":
                        result_df = self._apply_replace_strategy(result_df, source_df, target_col, available_sources[0])
                    
                    elif strategy_type == "prompt_merge":
                        prompt_template = field_strategy.get('prompt_template', '')
                        result_df = self._apply_prompt_merge_strategy(result_df, source_df, target_col, available_sources, prompt_template)
                    
                    elif strategy_type == "concat":
                        result_df = self._apply_concat_strategy(result_df, source_df, target_col, available_sources)
                    
                    elif strategy_type == "derive":
                        transformation_rule = field_strategy.get('transformation_rule', '')
                        result_df = self._apply_derive_strategy(result_df, source_df, target_col, available_sources, transformation_rule)
                    
                    elif strategy_type == "transform":
                        transformation_rule = field_strategy.get('transformation_rule', '')
                        result_df = self._apply_transform_strategy(result_df, source_df, target_col, available_sources[0], transformation_rule)
                    
                    elif strategy_type == "preserve":
                        print(f"      ‚è≠Ô∏è  Preserving existing data, ignoring source")
                    
                    else:
                        print(f"      ‚ö†Ô∏è  Unknown strategy '{strategy_type}', falling back to replace")
                        result_df = self._apply_replace_strategy(result_df, source_df, target_col, available_sources[0])
                        
                    print(f"      ‚úÖ Field processing complete")
                    
                except Exception as e:
                    print(f"      ‚ùå Field processing failed: {e}")
                    print(f"      üîÑ Skipping field and continuing...")
            
            else:
                print(f"      ‚ö†Ô∏è  No source mapping defined - field will remain empty")
        
        # Handle any unmapped source data
        unmapped_columns = strategy.get('unmapped_source_columns', {})
        if unmapped_columns:
            print(f"\n   üì¶ Processing {len(unmapped_columns)} unmapped source columns:")
            for source_col, unmapped_info in unmapped_columns.items():
                action = unmapped_info.get('action', 'ignore')
                reason = unmapped_info.get('reason', 'No reason provided')
                print(f"      ‚Ä¢ {source_col}: {action}")
                print(f"        üí≠ Reason: {reason[:50]}...")
                
                if action == "store_as_metadata" and source_col in source_df.columns:
                    # Add as additional column
                    result_df[f"meta_{source_col}"] = source_df[source_col] if len(source_df) == len(result_df) else None
                    print(f"        ‚úÖ Stored as meta_{source_col}")
        
        # Apply data quality rules
        quality_rules = strategy.get('data_quality_rules', {})
        if quality_rules:
            print(f"\n   üîç Applying data quality rules...")
            result_df = self._apply_quality_rules(result_df, quality_rules)
            print(f"      ‚úÖ Quality rules applied")
        
        final_rows = len(result_df)
        final_cols = len(result_df.columns)
        non_empty_cols = sum(1 for col in result_df.columns if result_df[col].notna().any())
        
        print(f"\n‚úÖ Field Processing Complete:")
        print(f"   üìä Output dimensions: {final_rows} rows √ó {final_cols} columns")
        print(f"   üìà Populated fields: {non_empty_cols}/{final_cols}")
        print(f"   üéØ Schema compliance: Target structure maintained")
        
        return result_df
    
    def _apply_replace_strategy(self, target_df: pd.DataFrame, source_df: pd.DataFrame, 
                               target_col: str, source_col: str) -> pd.DataFrame:
        """Replace target column with source column data"""
        
        if target_df.empty:
            # Start with empty dataframe but use source length
            target_df = pd.DataFrame(index=range(len(source_df)))
        
        # Handle different row counts
        if len(source_df) > len(target_df):
            # Extend target_df to accommodate all source rows
            additional_rows = len(source_df) - len(target_df)
            empty_rows = pd.DataFrame([{col: None for col in target_df.columns}] * additional_rows)
            target_df = pd.concat([target_df, empty_rows], ignore_index=True)
        
        # Now safely assign - truncate source if it's longer than target
        source_values = source_df[source_col].values
        if len(source_values) > len(target_df):
            source_values = source_values[:len(target_df)]
        elif len(source_values) < len(target_df):
            # Pad with None if source is shorter
            padded_values = np.full(len(target_df), None, dtype=object)
            padded_values[:len(source_values)] = source_values
            source_values = padded_values
        
        target_df[target_col] = source_values
        non_null_count = pd.Series(source_values).notna().sum()
        print(f"        ‚úÖ Replaced with {source_col} ({non_null_count}/{len(source_values)} non-null values)")
        return target_df
    
    def _apply_prompt_merge_strategy(self, target_df: pd.DataFrame, source_df: pd.DataFrame,
                                    target_col: str, source_cols: List[str], prompt_template: str) -> pd.DataFrame:
        """Use LLM to intelligently merge target and source data"""
        
        # For now, implement a simplified version - in production this would use LLM for each row
        print(f"        ü§ñ Applying LLM merge strategy (using concat fallback)")
        return self._apply_concat_strategy(target_df, source_df, target_col, source_cols)
    
    def _apply_concat_strategy(self, target_df: pd.DataFrame, source_df: pd.DataFrame,
                              target_col: str, source_cols: List[str]) -> pd.DataFrame:
        """Concatenate target and source values"""
        
        if target_df.empty:
            target_df = pd.DataFrame()
        
        # Extend target_df if needed
        if len(source_df) > len(target_df):
            additional_rows = len(source_df) - len(target_df)
            empty_rows = pd.DataFrame([{col: None for col in target_df.columns}] * additional_rows)
            target_df = pd.concat([target_df, empty_rows], ignore_index=True)
        
        # Concatenate source columns
        source_combined = source_df[source_cols].fillna('').astype(str).agg(' '.join, axis=1)
        
        if target_col in target_df.columns:
            # Combine with existing data
            existing_data = target_df[target_col].fillna('').astype(str)
            target_df[target_col] = (existing_data + ' | ' + source_combined).str.strip(' |')
        else:
            target_df[target_col] = source_combined
        
        non_empty_count = target_df[target_col].str.strip().ne('').sum()
        print(f"        ‚úÖ Concatenated {len(source_cols)} columns ({non_empty_count} non-empty results)")
        return target_df
    
    def _apply_derive_strategy(self, target_df: pd.DataFrame, source_df: pd.DataFrame,
                              target_col: str, source_cols: List[str], transformation_rule: str) -> pd.DataFrame:
        """Derive new values by combining multiple source columns"""
        
        if target_df.empty:
            target_df = pd.DataFrame()
        
        # Extend target_df if needed
        if len(source_df) > len(target_df):
            additional_rows = len(source_df) - len(target_df)
            empty_rows = pd.DataFrame([{col: None for col in target_df.columns}] * additional_rows)
            target_df = pd.concat([target_df, empty_rows], ignore_index=True)
        
        # Apply derivation logic (simplified - could be enhanced with more rules)
        if "first_last_name" in transformation_rule.lower() and len(source_cols) >= 2:
            first_name = source_df[source_cols[0]].fillna('')
            last_name = source_df[source_cols[1]].fillna('')
            target_df[target_col] = (first_name + ' ' + last_name).str.strip()
            complete_names = target_df[target_col].str.strip().ne('').sum()
            print(f"        ‚úÖ Derived full names from {source_cols[0]} + {source_cols[1]} ({complete_names} complete)")
        else:
            # Default: concatenate with space
            derived_values = source_df[source_cols].fillna('').astype(str).agg(' '.join, axis=1)
            target_df[target_col] = derived_values
            non_empty_count = derived_values.str.strip().ne('').sum()
            print(f"        ‚úÖ Derived from {len(source_cols)} columns ({non_empty_count} non-empty)")
        
        return target_df
    
    def _apply_transform_strategy(self, target_df: pd.DataFrame, source_df: pd.DataFrame,
                                 target_col: str, source_col: str, transformation_rule: str) -> pd.DataFrame:
        """Apply specific transformation to source data"""
        
        if target_df.empty:
            target_df = pd.DataFrame()
        
        # Extend target_df if needed
        if len(source_df) > len(target_df):
            additional_rows = len(source_df) - len(target_df)
            empty_rows = pd.DataFrame([{col: None for col in target_df.columns}] * additional_rows)
            target_df = pd.concat([target_df, empty_rows], ignore_index=True)
        
        source_data = source_df[source_col]
        original_count = source_data.notna().sum()
        
        # Apply transformations based on rules
        if "lowercase" in transformation_rule.lower():
            transformed_data = source_data.astype(str).str.lower()
            print(f"        ‚úÖ Applied lowercase transformation ({original_count} values)")
        elif "standardize_email" in transformation_rule.lower():
            transformed_data = source_data.astype(str).str.strip().str.lower()
            print(f"        ‚úÖ Applied email standardization ({original_count} values)")
        elif "format_phone" in transformation_rule.lower():
            # Basic phone formatting
            transformed_data = source_data.astype(str).str.replace(r'[^\d]', '', regex=True)
            print(f"        ‚úÖ Applied phone formatting ({original_count} values)")
        else:
            transformed_data = source_data
            print(f"        ‚ö†Ô∏è  No specific transformation rule applied ({original_count} values)")
        
        target_df[target_col] = transformed_data.values[:len(target_df)]
        return target_df
    
    def _apply_quality_rules(self, df: pd.DataFrame, quality_rules: Dict) -> pd.DataFrame:
        """Apply data quality rules to the final dataframe"""
        
        validation = quality_rules.get('validation', {})
        cleanup = quality_rules.get('cleanup', {})
        
        rules_applied = 0
        
        # Email validation
        if validation.get('email_validation') and 'email' in df.columns:
            email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
            valid_emails = df['email'].str.match(email_pattern, na=False)
            invalid_count = (~valid_emails).sum()
            total_emails = df['email'].notna().sum()
            if invalid_count > 0:
                print(f"      üìß Email validation: {total_emails - invalid_count}/{total_emails} valid addresses")
            else:
                print(f"      ‚úÖ Email validation: All {total_emails} addresses valid")
            rules_applied += 1
        
        # Phone formatting
        if validation.get('phone_formatting') and 'phone' in df.columns:
            phone_count = df['phone'].notna().sum()
            print(f"      üìû Phone formatting applied to {phone_count} numbers")
            rules_applied += 1
        
        # Name standardization
        if validation.get('name_standardization'):
            name_columns = [col for col in df.columns if 'name' in col.lower()]
            if name_columns:
                for col in name_columns:
                    df[col] = df[col].astype(str).str.title()
                print(f"      üë§ Name standardization applied to {len(name_columns)} columns")
                rules_applied += 1
        
        # Whitespace cleanup
        if cleanup.get('trim_whitespace'):
            text_columns = df.select_dtypes(include=['object']).columns
            for col in text_columns:
                df[col] = df[col].astype(str).str.strip()
            print(f"      üßπ Whitespace trimmed from {len(text_columns)} text columns")
            rules_applied += 1
        
        # Format standardization
        if cleanup.get('standardize_formats'):
            print(f"      üìê Format standardization applied")
            rules_applied += 1
        
        # Handle nulls
        null_handling = cleanup.get('handle_nulls', 'preserve')
        if null_handling == 'replace_with_empty':
            null_count = df.isnull().sum().sum()
            df = df.fillna('')
            print(f"      üîÑ Replaced {null_count} null values with empty strings")
            rules_applied += 1
        elif null_handling == 'skip':
            initial_rows = len(df)
            df = df.dropna()
            removed_rows = initial_rows - len(df)
            if removed_rows > 0:
                print(f"      üóëÔ∏è  Removed {removed_rows} rows with null values")
            rules_applied += 1
        
        if rules_applied == 0:
            print(f"      ‚ÑπÔ∏è  No quality rules specified to apply")
        else:
            print(f"      ‚úÖ Applied {rules_applied} data quality rules")
        
        return df
    
    def create_table_config(self, table_path: str, purpose: str,
                          enrichment_columns: Union[List[str], Dict[str, str]] = None,
                          column_policy: str = "balanced") -> TableConfig:
        """Helper to create and save a table configuration"""
        
        if enrichment_columns is None:
            if Path(table_path).exists():
                df = pd.read_csv(table_path)
                # Create enhanced format with column descriptions
                enrichment_columns = {}
                for col in list(df.columns[:5]):
                    enrichment_columns[col] = f"Key business field: {col}"
            else:
                # Default customer structure
                enrichment_columns = {
                    "email": "Primary email address for contact",
                    "full_name": "Full name of the contact",
                    "company_name": "Company or organization name",
                    "phone": "Primary phone number",
                    "notes": "Additional notes and context"
                }
        
        # Convert legacy list format to enhanced dict format
        if isinstance(enrichment_columns, list):
            enrichment_dict = {}
            for col in enrichment_columns:
                enrichment_dict[col] = f"Important field: {col}"
            enrichment_columns = enrichment_dict
        
        config = TableConfig(
            purpose=purpose,
            enrichment_columns=enrichment_columns,
            column_policy=column_policy,  # Support free-form text
            target=table_path  # Use the table path as the target
        )
        
        # Save configuration
        config_path = f"{Path(table_path).stem}_config.yaml"
        config.to_yaml(config_path)
        
        print(f"üìù Created configuration file: {config_path}")
        print(f"üìã Format: Enhanced with column descriptions")
        print(f"üéØ Enrichment columns: {len(enrichment_columns)} specified")
        print(f"üìú Column policy: {column_policy}")
        print(f"üìÇ Target file: {table_path}")
        return config

def main():
    """CLI interface for intelligent data ingestion"""
    
    if len(sys.argv) < 2:
        print("""
üöÄ Intelligent Data Ingestion - LLM-Powered CSV Mapping

Usage: python adaptive_merger.py <yaml_config> <csv_file>

Examples:
  python adaptive_merger.py schema.yaml data.csv
  python adaptive_merger.py customer_config.yaml apollo-export.csv

Alternative Commands:
  python adaptive_merger.py config <table> <purpose>   - Create YAML configuration  
  python adaptive_merger.py analyze <csv>              - Analyze unknown CSV structure

Key Features:
  ‚úÖ YAML-driven target schema definition with output location
  ‚úÖ Automatic unknown CSV structure detection
  ‚úÖ Intelligent field mapping with confidence scores
  ‚úÖ LLM-powered merge strategies (replace, derive, concat, etc.)
  ‚úÖ Policy-driven data quality enforcement
        """)
        sys.exit(1)
    
    command_or_yaml = sys.argv[1]
    merger = AdaptiveMerger()
    
    # Check if it's a YAML file (main ingestion command)
    if command_or_yaml.endswith('.yaml') or command_or_yaml.endswith('.yml'):
        if len(sys.argv) < 3:
            print("‚ùå CSV file required for ingestion")
            print("Usage: python adaptive_merger.py <yaml_config> <csv_file>")
            sys.exit(1)
        
        yaml_config = command_or_yaml
        csv_file = sys.argv[2]
        
        if not Path(yaml_config).exists():
            print(f"‚ùå YAML configuration file not found: {yaml_config}")
            sys.exit(1)
            
        if not Path(csv_file).exists():
            print(f"‚ùå CSV file not found: {csv_file}")
            sys.exit(1)
        
        result = merger.ingest_csv(yaml_config, csv_file)
        print(f"üéâ Success! Ingested {len(result)} records with {len(result.columns)} columns")
    
    # Handle other commands
    elif command_or_yaml == "config":
        if len(sys.argv) < 4:
            print("‚ùå Table file and purpose required for config command")
            print("Usage: python adaptive_merger.py config <table> <purpose>")
            sys.exit(1)
        
        table = sys.argv[2]
        purpose = sys.argv[3]
        
        config = merger.create_table_config(table, purpose)
        print(f"‚úÖ Configuration created for {table}")
    
    elif command_or_yaml == "analyze":
        if len(sys.argv) < 3:
            print("‚ùå CSV file required for analyze command")
            print("Usage: python adaptive_merger.py analyze <csv>")
            sys.exit(1)
        
        csv_file = sys.argv[2]
        
        if not Path(csv_file).exists():
            print(f"‚ùå CSV file {csv_file} not found")
            sys.exit(1)
        
        analysis = merger.analyze_unknown_csv(csv_file)
        print(f"\nüìä CSV Analysis Results:")
        print(f"   üéØ Purpose: {analysis.get('table_purpose', 'Unknown')}")
        print(f"   üì¶ Source: {analysis.get('data_source', 'Unknown')}")
        print(f"   üìä Dimensions: {analysis.get('row_count', 0)} rows √ó {analysis.get('column_count', 0)} columns")
        
        if 'source_identification' in analysis:
            platform_conf = analysis['source_identification'].get('platform_confidence', 0)
            export_type = analysis['source_identification'].get('export_type', 'unknown')
            print(f"   üîç Platform confidence: {platform_conf:.2f} ({export_type})")
        
        print(f"\nüìã Column Analysis:")
        for col, details in analysis.get('column_semantics', {}).items():
            semantic_type = details.get('semantic_type', 'unknown')
            confidence = details.get('confidence', 0)
            print(f"   ‚Ä¢ {col}: {semantic_type} (confidence: {confidence:.2f})")
    
    else:
        print(f"‚ùå Invalid usage. Expected YAML file or command (config/analyze)")
        print("Usage: python adaptive_merger.py <yaml_config> <csv_file>")
        print("   or: python adaptive_merger.py config <table> <purpose>")
        print("   or: python adaptive_merger.py analyze <csv>")
        sys.exit(1)

if __name__ == "__main__":
    main() 